{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Setup and Hyperparameter Initialization\n",
        "\n",
        "This part lays the groundwork for the project by preparing the environment, defining dataset paths, and configuring key parameters for the image captioning model."
      ],
      "metadata": {
        "id": "-4mc6TRl-vMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive\n",
        "\n",
        "We mounted Google Drive to access the necessary datasets and files directly from the cloud storage. Change this according to the location of dataset.\n",
        "\n",
        "###Library Imports\n",
        "\n",
        "* Core libraries like **os** (file system operations), **re** (regular expressions), **numpy** (numerical computations), and **tensorflow/keras** (deep learning framework) are imported.\n",
        "\n",
        "* Additional tools like **sklearn** are imported for splitting datasets into training and testing subsets.\n",
        "\n",
        "###Image and Sequence Configuration\n",
        "\n",
        "* **IMAGE_SIZE:** Images are resized to dimensions of (299, 299) to standardize inputs for the neural network.\n",
        "\n",
        "* **SEQ_LENGTH:** Sets a fixed maximum length of 25 tokens for textual sequences (captions). This ensures consistency across data batches.\n",
        "\n",
        "###Vocabulary and Embedding Dimensions\n",
        "\n",
        "* **VOCAB_SIZE:** Limited the vocabulary size to 10,000 words to manage the language model complexity while maintaining a diverse enough vocabulary for meaningful captions.\n",
        "\n",
        "* **EMBED_DIM:** Set the embedding dimensions for both image and text features to 512, ensuring the model can learn compact yet expressive representations.\n",
        "\n",
        "###Transformer Model Parameters\n",
        "\n",
        "* **FF_DIM:** Defines the size of the feed-forward network within the transformer. Configured the feed-forward network size to 512 units, balancing complexity and computational efficiency.\n",
        "\n",
        "* **NUM_HEADS:** Configures the number of attention heads in the transformer architecture for multi-head attention. Set the number of attention heads to 2 for the multi-head attention mechanism in the transformer.\n",
        "\n",
        "###Training Parameters:\n",
        "\n",
        "* **BATCH_SIZE:** Specifies the number of samples processed simultaneously during training. We set the batch size to 256, which determines the number of samples processed simultaneously during training.\n",
        "\n",
        "* **EPOCHS:** Sets the number of complete passes through the dataset for model training, here defined as 30."
      ],
      "metadata": {
        "id": "HHBRMh3z_ETL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXsQAnWR-Kk5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.applications import efficientnet\n",
        "from keras.layers import TextVectorization\n",
        "from keras.utils import register_keras_serializable\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define constants\n",
        "IMAGES_PATH = \"/content/drive/MyDrive/Colab/flickr30k_images\"\n",
        "CAPTIONS_PATH = \"/content/drive/MyDrive/Colab/results.csv\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Number of units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Number of attention heads\n",
        "NUM_HEADS = 2\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Number of epochs\n",
        "EPOCHS = 30\n"
      ],
      "metadata": {
        "id": "f4Gd1_Gr-gxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading, Preprocessing, and Splitting\n",
        "\n",
        "In this section, we have focused on:\n",
        "\n",
        "* Loading and preprocessing the captions dataset to prepare it for model training.\n",
        "* Filtering and validating the data to ensure captions meet the specified length constraints.\n",
        "* Splitting the dataset into training, validation, and test subsets to enable robust model evaluation."
      ],
      "metadata": {
        "id": "Vf9f7_oJDSQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(load_captions_data) Function:\n",
        "\n",
        "* **Purpose:** To load and preprocess the captions dataset from a file.\n",
        "\n",
        "* **Process:** Opens the captions file specified by filename and reads its lines, skipping the header.\n",
        "\n",
        " *Initializes:*\n",
        "\n",
        " * **(caption_mapping):** A dictionary mapping image file paths to their corresponding captions.\n",
        " * **(text_data):** A list of all processed captions.\n",
        " * **(images_to_skip):** A set to track images that should be excluded based on caption length criteria.\n",
        "\n",
        " *Iterates over each line in the file:*\n",
        "\n",
        " * Splits the line into the image name and its associated caption. Handles exceptions for malformed entries.\n",
        " * Filters out captions that are too short (< 4 tokens) or too long **(> SEQ_LENGTH)** and adds such images to the **images_to_skip** set.\n",
        " * Appends captions to **text_data** after adding <**start**> and <**end**> tokens to mark the beginning and end of the captions.\n",
        " * Updates **caption_mapping** by appending captions to the corresponding image path.\n",
        "\n",
        " *Removes skipped images from caption_mapping to ensure only valid entries remain.*\n",
        "\n",
        "* **Returns:**\n",
        "\n",
        " * **caption_mapping:** Dictionary mapping image paths to captions.\n",
        "\n",
        " * **text_data:** List of processed captions.\n",
        "\n",
        "###(train_val_split) Function:\n",
        "\n",
        "* **Purpose:** To split the dataset into training, validation, and test subsets.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Extracts all image keys (all_images) from caption_data.\n",
        " * Optionally shuffles the image keys for randomization.\n",
        " * Splits the dataset: First into training and validation sets based on **validation_size**. Then splits the validation set further into validation and test subsets based on **test_size**.\n",
        " * Creates new dictionaries (**training_data, validation_data, and test_data**) mapping image paths to captions for each subset.\n",
        "\n",
        "* **Returns:**\n",
        "\n",
        " * **training_data:** Training subset.\n",
        " * **validation_data:** Validation subset.\n",
        " * **test_data:** Test subset.\n",
        "\n",
        "###Loading and Splitting the Dataset:\n",
        "\n",
        "* Calls **load_captions_data** to load the captions and map them to images.\n",
        "\n",
        "* Calls **train_val_split** to partition the dataset into training, validation, and test sets.\n",
        "\n",
        "* Prints the total number of samples and the count for each subset."
      ],
      "metadata": {
        "id": "xD7X6JLMENzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions_data(filename):\n",
        "    with open(filename, encoding='utf-8') as caption_file:\n",
        "        caption_data = caption_file.readlines()[1:]\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        images_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            try:\n",
        "                img_name, _, caption = line.split(\"| \")\n",
        "            except ValueError:\n",
        "                img_name, caption = line.split(\"| \")\n",
        "                caption = caption[4:]\n",
        "\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "            tokens = caption.strip().split()\n",
        "            if len(tokens) < 4 or len(tokens) > SEQ_LENGTH:\n",
        "                images_to_skip.add(img_name)\n",
        "                continue\n",
        "\n",
        "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        for img_name in images_to_skip:\n",
        "            if img_name in caption_mapping:\n",
        "                del caption_mapping[img_name]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "def train_val_split(caption_data, validation_size=0.2, test_size=0.02, shuffle=True):\n",
        "    all_images = list(caption_data.keys())\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    train_keys, validation_keys = train_test_split(\n",
        "        all_images, test_size=validation_size, random_state=42\n",
        "    )\n",
        "    validation_keys, test_keys = train_test_split(\n",
        "        validation_keys, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    training_data = {img_name: caption_data[img_name] for img_name in train_keys}\n",
        "    validation_data = {img_name: caption_data[img_name] for img_name in validation_keys}\n",
        "    test_data = {img_name: caption_data[img_name] for img_name in test_keys}\n",
        "\n",
        "    return training_data, validation_data, test_data\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(CAPTIONS_PATH)\n",
        "\n",
        "# Split the dataset\n",
        "train_data, validation_data, test_data = train_val_split(captions_mapping)\n",
        "\n",
        "print(f\"Total samples: {len(captions_mapping)}\")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(validation_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "OBGoZd9dEffG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Vectorization and Custom Preprocessing\n",
        "\n",
        "In this section, we have focused on:\n",
        "\n",
        "* Defining a custom preprocessing pipeline to clean and standardize the captions, ensuring they are consistent and free from unnecessary characters.\n",
        "\n",
        "* Configuring a **TextVectorization** layer to tokenize the captions and convert them into fixed-length integer sequences.\n",
        "\n",
        "* Adapting the vectorizer to the processed text data to generate a vocabulary and prepare the captions for input into the model."
      ],
      "metadata": {
        "id": "30t_GdOyHsFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(custom_standardization) Function:\n",
        "\n",
        "* **Purpose:** To preprocess and clean text input by standardizing it before tokenization.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Converts the input string to lowercase using **tf.strings.lower()** to ensure uniformity.\n",
        " * Defines a set of characters to strip **(strip_chars)**, which includes punctuation, special characters, and digits.\n",
        " * Uses **tf.strings.regex_replace()** to remove all occurrences of the specified **strip_chars** from the input text.\n",
        "\n",
        "* **Returns:** A cleaned, lowercase version of the input text with specified characters removed.\n",
        "\n",
        "###Text Vectorizer (TextVectorization Layer):\n",
        "\n",
        "* **Purpose:** Converts raw text into sequences of integers for model training.\n",
        "\n",
        "* **Configuration:**\n",
        "\n",
        " * **max_tokens:** Limits the vocabulary size to **VOCAB_SIZE** (10,000 words) to control complexity.\n",
        " * **output_mode:** Set to **\"int\"**, so text is tokenized and mapped to integer sequences.\n",
        " * **output_sequence_length:** Pads or truncates sequences to a fixed length of **SEQ_LENGTH** (25 tokens) for uniformity.\n",
        "\n",
        " * **standardize:** Applies the custom preprocessing function **(custom_standardization)** to clean the text before tokenization.\n",
        "\n",
        "###Adapting the Vectorizer:\n",
        "\n",
        "* **Purpose:** To build the vocabulary for the text data.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * **vectorization.adapt(text_data)** is called, where **text_data** is the list of all cleaned captions from the previous step.\n",
        " * This step analyzes the text data and prepares a vocabulary of the most frequent **VOCAB_SIZE** words, assigning an integer index to each word."
      ],
      "metadata": {
        "id": "wI_cA6ZUHtZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    strip_chars = \"!\\\"#$%&'()*+,-./:;=?@[\\]^_`{|}~1234567890\"\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "# Define the vectorizer\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization\n",
        ")\n",
        "\n",
        "# Adapt the vectorizer to the text data\n",
        "vectorization.adapt(text_data)"
      ],
      "metadata": {
        "id": "QRqHvTR6HpQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Augmentation Pipeline\n",
        "\n",
        "In this section, we have created an image augmentation pipeline to:\n",
        "\n",
        "* Enhance training data diversity by simulating real-world variations in images.\n",
        "\n",
        "* Improve the robustness and generalization of the image captioning model by exposing it to transformed versions of the same images.\n",
        "\n"
      ],
      "metadata": {
        "id": "kN0UaS56Liko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(image_augmentation) Definition:\n",
        "\n",
        "* **Purpose:** To enhance the training dataset by applying random transformations to images, improving the model's ability to generalize to new data.\n",
        "\n",
        "* **Process:** A sequential model is created using **keras.Sequential** to apply a series of image augmentations.\n",
        "\n",
        " *Augmentation Layers:*\n",
        "\n",
        "  * **layers.RandomFlip(\"horizontal\"):** Randomly flips images horizontally, simulating different orientations.\n",
        "  * **layers.RandomRotation(0.2):** Randomly rotates images within a range of ±20% (of 360 degrees), simulating various angles of view.\n",
        "  * **layers.RandomContrast(0.3):** Adjusts image contrast randomly by up to ±30%, simulating varying lighting conditions.\n",
        "\n",
        "* **Integration:** The **image_augmentation** pipeline can be applied to images during training to increase diversity in the dataset and make the model more robust to variations in image orientation, rotation, and contrast."
      ],
      "metadata": {
        "id": "qVQWDHuJLz4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomContrast(0.3)\n",
        "])"
      ],
      "metadata": {
        "id": "ROat2lsyHuev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Transformer Encoder Block\n",
        "\n",
        "In this section, we have implemented a custom transformer encoder block to:\n",
        "\n",
        "* Extract high-level contextual features from sequential input data using attention mechanisms.\n",
        "\n",
        "* Enhance model performance by enabling it to focus on relevant parts of the sequence dynamically.\n",
        "\n",
        "* Integrate normalization and feed-forward layers for improved training stability and expressive power."
      ],
      "metadata": {
        "id": "_gWJXPgZHw1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining TransformerEncoderBlock:\n",
        "\n",
        "* **Purpose:** Implements a custom transformer encoder block for processing sequential data, such as image embeddings or text token embeddings.\n",
        "\n",
        "* **@register_keras_serializable:** Registers the layer as a custom Keras serializable object, allowing the model to save and reload this layer seamlessly.\n",
        "\n",
        "###init Method:\n",
        "\n",
        "* Initializes the transformer encoder block by defining the following sub-layers:\n",
        "\n",
        " **MultiHeadAttention (self.attention_1):** A multi-head attention mechanism with:\n",
        " * **num_heads:** Number of attention heads (defined as **self.num_heads**).\n",
        " * **key_dim:** Dimensionality of the keys (set as **self.embed_dim**).\n",
        " * Allows the model to focus on different parts of the input sequence simultaneously.\n",
        "\n",
        " **LayerNormalization (self.layernorm_1 and self.layernorm_2):** Normalizes inputs to stabilize training and prevent gradient explosion/vanishing.\n",
        "\n",
        " **Dense Layer (self.dense_1):** A feed-forward dense layer with ReLU activation applied to the normalized input. The dimensionality is set to **self.embed_dim**.\n",
        "\n",
        "###call Method:\n",
        "\n",
        "* **Purpose:** Defines the forward pass of the encoder block.\n",
        "\n",
        "* **Process:**\n",
        " 1. Normalizes the input embeddings using **self.layernorm_1**.\n",
        " 2. Passes the normalized inputs through the dense layer **(self.dense_1)**.\n",
        " 3. Applies multi-head attention using **self.attention_1**, where the queries, keys, and values are all derived from the dense layer outputs.\n",
        " 4. Combines the dense layer output with the attention output (via skip connection) and applies a second normalization **(self.layernorm_2)**.\n",
        "\n",
        "* **Returns:** The final output of the encoder block.\n",
        "\n",
        "###get_config Method:\n",
        "\n",
        "* **Purpose:** Ensures the custom layer can be serialized and deserialized.\n",
        "\n",
        "* **Process:** Updates the configuration dictionary with the block's parameters: **embed_dim, dense_dim,** and **num_heads**."
      ],
      "metadata": {
        "id": "tvYDZYx5Mh_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Initialize sub-layers in __init__\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.dense_1 = layers.Dense(self.embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs_norm = self.layernorm_1(inputs)\n",
        "        inputs_dense = self.dense_1(inputs_norm)\n",
        "        attention_output = self.attention_1(\n",
        "            query=inputs_dense, value=inputs_dense, key=inputs_dense, training=training\n",
        "        )\n",
        "        out = self.layernorm_2(inputs_dense + attention_output)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerEncoderBlock, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "GpYw_XrkHxy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Embedding Layer for Token and Sequence Representation\n",
        "\n",
        "In this section, we have implemented a **PositionalEmbedding** layer to:\n",
        "\n",
        "* Embed input tokens (words) as dense vectors.\n",
        "\n",
        "* Incorporate positional information to preserve sequence order in the transformer model.\n",
        "\n",
        "* Scale embeddings for numerical stability and compute masks to handle padding tokens."
      ],
      "metadata": {
        "id": "CGOI5zwrH1aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining PositionalEmbedding:\n",
        "\n",
        "* **Purpose:** Implements a custom layer to combine token embeddings and positional embeddings, which are critical for maintaining sequence order in transformer models.\n",
        "\n",
        "* **@register_keras_serializable:** Registers this layer as a custom Keras serializable component, ensuring compatibility for saving and loading models.\n",
        "\n",
        "\n",
        "###init Method:\n",
        "\n",
        "* **Components:**\n",
        "\n",
        " * **token_embeddings:** An embedding layer that converts input tokens (words) into dense vectors of size **embed_dim**.\n",
        " * **position_embeddings:** An embedding layer that assigns a positional vector to each token position in the sequence, with the same size **(embed_dim)**.\n",
        " * **Parameters:**  \n",
        "  \n",
        "   1. **sequence_length:** Maximum number of tokens in a sequence.\n",
        "   2. **vocab_size:** Total number of unique tokens in the vocabulary.\n",
        "   3. **embed_dim:** Dimensionality of the embedding vectors.\n",
        "\n",
        "###call Method:\n",
        "\n",
        "* **Purpose:** Combines token embeddings with positional embeddings to incorporate sequence order information.\n",
        "\n",
        "* **Process:**\n",
        " 1. Calculates the sequence length dynamically from the input using **tf.shape(inputs)[-1]**.\n",
        " 2. Generates a range of positions **(positions)** for the tokens in the sequence (e.g., [0, 1, 2, ...]).\n",
        " 3. Applies the token embedding layer **(self.token_embeddings)** to convert input tokens into dense vectors and scales the embeddings by the square root of **embed_dim (embed_scale = sqrt(embed_dim))** for stability.\n",
        " 4. Generates positional embeddings **(self.position_embeddings)** for the positions.\n",
        " 5. Combines the scaled token embeddings and positional embeddings via element-wise addition.\n",
        "\n",
        "* **Returns:** A tensor of shape **(batch_size, sequence_length, embed_dim)** containing the combined embeddings.\n",
        "\n",
        "###compute_mask Method:\n",
        "\n",
        "* **Purpose:** Computes a mask to ignore padding tokens (tokens with value 0) during training.\n",
        "\n",
        "* **Process:** Uses **tf.math.not_equal(inputs, 0)** to generate a boolean mask indicating which tokens are non-padding tokens.\n",
        "\n",
        "###get_config Method:\n",
        "\n",
        "* **Purpose:** Ensures the layer's parameters are included for serialization.\n",
        "\n",
        "* **Process:** Updates the configuration dictionary with **sequence_length, vocab_size,** and **embed_dim**."
      ],
      "metadata": {
        "id": "Pllop3PfPNqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        # Remove 'embed_scale' from __init__\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embed_dim = tf.cast(self.embed_dim, tf.float32)\n",
        "        embed_scale = tf.math.sqrt(embed_dim)\n",
        "        embedded_tokens = self.token_embeddings(inputs) * embed_scale\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "BBhWkC9IH2Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Transformer Decoder Block\n",
        "\n",
        "In this section, we have built a custom transformer decoder block to:\n",
        "\n",
        "* Process input sequences with attention mechanisms that integrate encoded inputs and prior outputs.\n",
        "\n",
        "* Maintain autoregressive behavior using causal masking.\n",
        "\n",
        "* Generate token probabilities for caption generation in the image captioning model."
      ],
      "metadata": {
        "id": "cdDopXpbH5DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining TransformerDecoderBlock:\n",
        "\n",
        "* **Purpose:** Implements a custom transformer decoder block for generating sequential outputs, such as text captions, by leveraging encoded input and prior context.\n",
        "\n",
        "* **@register_keras_serializable:** Registers this layer as a custom Keras serializable component, ensuring compatibility for model saving and loading.\n",
        "\n",
        "###init Method:\n",
        "\n",
        "* Initializes the decoder block with the following components:\n",
        "\n",
        " **Attention Layers:**\n",
        "\n",
        " * **attention_1:** A self-attention mechanism allowing the model to focus on previously generated tokens.\n",
        " * **cross_attention_2:** A cross-attention mechanism that incorporates information from encoder outputs.\n",
        "\n",
        " **Feed-Forward Network:**\n",
        "\n",
        " * **ffn_layer_1** and **ffn_layer_2:** Dense layers for transforming and refining intermediate representations.\n",
        "\n",
        " **Normalization Layers:**\n",
        "\n",
        " * **layernorm_1, layernorm_2, layernorm_3:** Layer normalization to stabilize training and improve convergence.\n",
        "\n",
        " **Positional Embedding:**\n",
        "\n",
        " * **embedding:** Adds token and positional embeddings to input tokens for preserving sequence order.\n",
        "\n",
        " **Output Layer:**\n",
        "\n",
        " * **out:** A dense layer with a **softmax** activation that maps the output to probabilities over the vocabulary.\n",
        "\n",
        " **Dropout:**\n",
        "\n",
        " * **dropout_1** and **dropout_2:** Dropout layers to prevent overfitting.\n",
        "\n",
        " **Support for Masking:**\n",
        "\n",
        " * Enables handling of padding and causal masks to ensure proper attention behavior during training.\n",
        "\n",
        "###call Method:\n",
        "\n",
        "* **Purpose:** Defines the forward pass for the decoder block.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " 1. **Embedding:** Converts input tokens into dense embeddings using **self.embedding**.\n",
        " 2. **Causal Mask:** Generates a causal mask **(get_causal_attention_mask)** to ensure attention only attends to prior tokens, preserving autoregressive behavior.\n",
        " 3. **Self-Attention:** Applies self-attention **(self.attention_1)** to allow the model to focus on previously generated tokens. Then combines the result with the input embeddings using a residual connection and normalizes with **layernorm_1**.\n",
        " 4. **Cross-Attention:** Applies cross-attention **(self.cross_attention_2)** using the encoder outputs, allowing the decoder to leverage encoded input features. After that, adds the result to the output of the self-attention layer and normalizes with **layernorm_2**.\n",
        " 5. **Feed-Forward Network:** Passes the output through a feed-forward network **(ffn_layer_1 and ffn_layer_2)** with dropout and combines the result using residual connections and **layernorm_3**.\n",
        " 6. **Output:** Passes the refined output through the dense output layer **(self.out)** with a **softmax** activation to generate token probabilities.\n",
        "\n",
        "* **Returns:** Probabilities over the vocabulary for the next token.\n",
        "\n",
        "###get_causal_attention_mask Method:\n",
        "\n",
        "* **Purpose:** Creates a causal mask to prevent attention to future tokens.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Constructs a lower triangular matrix where each token can only attend to itself and previous tokens.\n",
        " * Expands the mask to match the batch size and sequence dimensions.\n",
        "\n",
        "###get_config Method:\n",
        "\n",
        "* **Purpose:** Ensures all parameters are serializable for model saving and loading.\n",
        "\n",
        "* **Process:** Updates the configuration dictionary with **embed_dim, ff_dim,** and **num_heads**."
      ],
      "metadata": {
        "id": "N8sDh1K9Sp2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoderBlock, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Initialize sub-layers in __init__\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
        "        )\n",
        "        self.cross_attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
        "        )\n",
        "        self.ffn_layer_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_layer_2 = layers.Dense(self.embed_dim)\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            sequence_length=SEQ_LENGTH,\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            embed_dim=EMBED_DIM\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs_embedded = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs_embedded)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "        else:\n",
        "            combined_mask = causal_mask\n",
        "\n",
        "        attention_output = self.attention_1(\n",
        "            query=inputs_embedded,\n",
        "            value=inputs_embedded,\n",
        "            key=inputs_embedded,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "        out1 = self.layernorm_1(inputs_embedded + attention_output)\n",
        "\n",
        "        cross_attention_output = self.cross_attention_2(\n",
        "            query=out1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask if mask is not None else None,\n",
        "            training=training\n",
        "        )\n",
        "        out2 = self.layernorm_2(out1 + cross_attention_output)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        batch_size, sequence_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
        "        return tf.tile(mask, [batch_size, 1, 1])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoderBlock, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'ff_dim': self.ff_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "bjCFMiBJH6GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction with Pre-trained CNN Model\n",
        "\n",
        "In this section, we have defined and instantiated a pre-trained CNN model for:\n",
        "\n",
        "* Extracting meaningful, high-level features from input images using EfficientNetB0.\n",
        "\n",
        "* Preparing image embeddings that will be processed by the transformer-based captioning model.\n"
      ],
      "metadata": {
        "id": "7LNXN0KIIIfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###get_cnn_model Function:\n",
        "\n",
        "* **Purpose:** Creates a Convolutional Neural Network (CNN) model for extracting high-level image features to be used as inputs for the image captioning transformer model.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " *Base Model:*\n",
        "\n",
        "  * Uses a pre-trained **EfficientNetB0** model from **keras.applications.efficientnet.**\n",
        "  * Configures the base model:\n",
        "\n",
        "   1. **input_shape:** Sets the input shape to **(*IMAGE_SIZE, 3)** (299x299 images with 3 color channels).\n",
        "   2. **include_top=False:** Excludes the fully connected classification head to focus only on feature extraction.\n",
        "   3. **weights=\"imagenet\":** Initializes the model with weights pre-trained on the ImageNet dataset for leveraging learned features.\n",
        "\n",
        " *Freezing the Base Model:*\n",
        "\n",
        "  * Sets **base_model.trainable = False** to freeze all layers of the EfficientNetB0 model, preventing weight updates during training and reducing computational load.\n",
        "\n",
        " *Reshaping the Output:*\n",
        "\n",
        "  * Reshapes the output of the CNN to **(-1, base_model_out.shape[-1])**. **-1** flattens the spatial dimensions (height and width). **base_model_out.shape[-1]** retains the feature depth (channel dimension).\n",
        "\n",
        " *Model Definition:*\n",
        "\n",
        "  * Creates the CNN model with **keras.models.Model**, specifying the base model's input and the reshaped output.\n",
        "\n",
        "* Instantiation:\n",
        "\n",
        " * **cnn_model:** Calls **get_cnn_model()** to instantiate the CNN model, which will be used to extract image features for subsequent processing in the transformer model."
      ],
      "metadata": {
        "id": "G_P8RmqMWS0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\"\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "cnn_model = get_cnn_model()"
      ],
      "metadata": {
        "id": "WENm3GdtIJeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instantiating Transformer Encoder and Decoder Blocks\n",
        "\n",
        "In this section, we have instantiated the core components of the transformer-based architecture:\n",
        "\n",
        "* **Encoder:** Processes and encodes image embeddings into meaningful feature representations.\n",
        "\n",
        "* **Decoder:** Decodes the sequence embeddings, integrating the encoder's outputs to predict captions token by token."
      ],
      "metadata": {
        "id": "84fIzI8mIL9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder Initialization:\n",
        "\n",
        "* **TransformerEncoderBlock:** Instantiates the custom encoder block with the following parameters:\n",
        "\n",
        " * **embed_dim:** Embedding dimensionality set to **EMBED_DIM** (512).\n",
        " * **dense_dim:** Dimensionality of the feed-forward network set to **FF_DIM** (512).\n",
        " * **num_heads:** Number of attention heads in the multi-head attention mechanism set to **NUM_HEADS** (2).\n",
        "\n",
        "The encoder processes image embeddings, enabling the model to extract high-level contextual representations through multi-head attention and feed-forward networks.\n",
        "\n",
        "###Decoder Initialization:\n",
        "\n",
        "* **TransformerDecoderBlock:** Instantiates the custom decoder block with the same parameters:\n",
        "\n",
        " * **embed_dim:** Embedding dimensionality set to **EMBED_DIM** (512).\n",
        " * **ff_dim:** Dimensionality of the feed-forward network set to **FF_DIM** (512).\n",
        " * **num_heads:** Number of attention heads in the self-attention and cross-attention mechanisms set to **NUM_HEADS** (2).\n",
        "\n",
        "The decoder processes sequence embeddings (token embeddings) and incorporates encoder outputs to generate meaningful captions."
      ],
      "metadata": {
        "id": "FK6EbRIUYaPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=NUM_HEADS)\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=NUM_HEADS)"
      ],
      "metadata": {
        "id": "tsKIiDgEIMok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Captioning Model Implementation\n",
        "\n",
        "In this section, we have implemented the complete image captioning model, integrating:\n",
        "\n",
        "* **Image Processing:** Feature extraction with the CNN.\n",
        "\n",
        "* **Sequence Modeling:** Image embedding processing with the encoder and caption generation with the decoder.\n",
        "\n",
        "* **Training/Evaluation Workflow:** Loss, accuracy calculation, and training/testing steps.\n",
        "\n",
        "* **Serialization:** Mechanisms for saving and loading the model configuration and weights."
      ],
      "metadata": {
        "id": "lbyFzlqZIaXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining ImageCaptioningModel:\n",
        "\n",
        "* **Purpose:** Implements a custom Keras model for end-to-end image captioning by combining the CNN, encoder, and decoder components with loss and accuracy tracking.\n",
        "\n",
        "###Initialization (init Method):\n",
        "\n",
        "* **cnn_model:** Pre-trained CNN for feature extraction.\n",
        "\n",
        "* **encoder:** Transformer encoder to process image embeddings.\n",
        "\n",
        "* **decoder:** Transformer decoder to generate captions.\n",
        "\n",
        "* **image_aug:** Optional image augmentation pipeline.\n",
        "\n",
        "* **Metrics:**\n",
        "\n",
        " * **loss_tracker:** Tracks the mean loss during training/testing.\n",
        " * **acc_tracker:** Tracks the mean accuracy during training/testing.\n",
        "\n",
        "###Forward Pass (call Method):\n",
        "\n",
        "* **Inputs:** A batch of images **(batch_img)** and their corresponding token sequences **(batch_seq)**.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " 1. Applies image augmentation **(image_aug)** during training, if provided.\n",
        " 2. Extracts image features using **cnn_model**.\n",
        " 3. Passes image embeddings through the encoder to generate **encoder_out**.\n",
        " 4. Splits input sequences into:\n",
        "   * **batch_seq_inp:** Input tokens for the decoder (all but the last token).\n",
        "   * **batch_seq_true:** Ground truth tokens (all but the first token).\n",
        " 5. Computes a mask for padding tokens in **batch_seq_inp**.\n",
        " 6. Passes input tokens and **encoder_out** through the decoder to generate predictions **(batch_seq_pred)**.\n",
        "\n",
        "###Loss Calculation (calculate_loss Method):\n",
        "\n",
        "* **Purpose:** Computes loss while ignoring padding tokens.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Uses the model's compiled loss function.\n",
        " * Multiplies the loss by the mask to ignore padding tokens.\n",
        " * Averages the loss over valid tokens.\n",
        "\n",
        "###Accuracy Calculation (calculate_accuracy Method):\n",
        "\n",
        "* **Purpose:** Computes sequence accuracy while ignoring padding tokens.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Compares predicted tokens **(y_pred)** with ground truth tokens **(y_true)**.\n",
        " * Applies the mask to ignore padding tokens.\n",
        " * Averages accuracy over valid tokens.\n",
        "\n",
        "###Training Step (train_step Method):\n",
        "\n",
        "* **Purpose:** Defines a single training step for the model.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " 1. Computes predictions **(batch_seq_pred)** and loss.\n",
        " 2. Backpropagates the loss using **GradientTape**.\n",
        " 3. Updates the encoder and decoder weights using the optimizer.\n",
        " 4. Tracks loss and accuracy using **loss_tracker** and **acc_tracker**.\n",
        "\n",
        "###Testing Step (test_step Method):\n",
        "\n",
        "* **Purpose:** Defines a single testing step for the model.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " 1. Computes predictions **(batch_seq_pred)** without updating weights.\n",
        " 2. Computes and tracks loss and accuracy using the same methods as **train_step**.\n",
        "\n",
        "###Metrics Property:\n",
        "\n",
        "* Ensures the model returns **loss_tracker** and **acc_tracker** during evaluation.\n",
        "\n",
        "###Serialization Methods:\n",
        "\n",
        "* **get_config:** Ensures model components **(cnn_model, encoder, decoder,** and **image_aug)** are serializable.\n",
        "\n",
        "* **from_config:** Deserializes components and reinstantiates the model for loading saved configurations."
      ],
      "metadata": {
        "id": "sm1qSnDLZh0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn_model,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        image_aug=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(ImageCaptioningModel, self).__init__(**kwargs)\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        batch_img, batch_seq = inputs\n",
        "\n",
        "        if self.image_aug and training:\n",
        "            batch_img = self.image_aug(batch_img)\n",
        "\n",
        "        img_embed = self.cnn_model(batch_img, training=False)\n",
        "        encoder_out = self.encoder(img_embed, training=training)\n",
        "\n",
        "        batch_seq_inp = batch_seq[:, :-1]\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "\n",
        "        mask = tf.math.not_equal(batch_seq_inp, 0)\n",
        "\n",
        "        batch_seq_pred = self.decoder(\n",
        "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "        )\n",
        "\n",
        "        return batch_seq_pred\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.compiled_loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            batch_seq_pred = self((batch_img, batch_seq), training=True)\n",
        "            batch_seq_true = batch_seq[:, 1:]\n",
        "            mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "            loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables +\n",
        "            self.decoder.trainable_variables\n",
        "        )\n",
        "        gradients = tape.gradient(loss, train_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, train_vars))\n",
        "\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_img, batch_seq = batch_data\n",
        "\n",
        "        batch_seq_pred = self((batch_img, batch_seq), training=False)\n",
        "        batch_seq_true = batch_seq[:, 1:]\n",
        "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
        "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ImageCaptioningModel, self).get_config()\n",
        "        config.update({\n",
        "            'cnn_model': keras.layers.serialize(self.cnn_model),\n",
        "            'encoder': keras.layers.serialize(self.encoder),\n",
        "            'decoder': keras.layers.serialize(self.decoder),\n",
        "            'image_aug': keras.layers.serialize(self.image_aug) if self.image_aug else None,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        cnn_model_config = config.pop('cnn_model')\n",
        "        encoder_config = config.pop('encoder')\n",
        "        decoder_config = config.pop('decoder')\n",
        "        image_aug_config = config.pop('image_aug', None)\n",
        "\n",
        "        cnn_model = keras.layers.deserialize(cnn_model_config, custom_objects={'EfficientNetB0': efficientnet.EfficientNetB0})\n",
        "        encoder = keras.layers.deserialize(encoder_config, custom_objects={'TransformerEncoderBlock': TransformerEncoderBlock})\n",
        "        decoder = keras.layers.deserialize(decoder_config, custom_objects={\n",
        "            'TransformerDecoderBlock': TransformerDecoderBlock,\n",
        "            'PositionalEmbedding': PositionalEmbedding\n",
        "        })\n",
        "        image_aug = keras.layers.deserialize(image_aug_config) if image_aug_config else None\n",
        "\n",
        "        return cls(\n",
        "            cnn_model=cnn_model,\n",
        "            encoder=encoder,\n",
        "            decoder=decoder,\n",
        "            image_aug=image_aug,\n",
        "            **config\n",
        "        )"
      ],
      "metadata": {
        "id": "GdL3Kq0QIbAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation and Dataset Creation\n",
        "\n",
        "In this section, we have created a robust pipeline for data preparation and dataset creation to:\n",
        "\n",
        "* Load and preprocess images and captions.\n",
        "\n",
        "* Organize and align data for efficient input into the model.\n",
        "\n",
        "* Optimize data loading with parallel processing, shuffling, batching, and prefetching."
      ],
      "metadata": {
        "id": "jU24xO7BId6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###decode_and_resize Function:\n",
        "\n",
        "* **Purpose:** Loads and preprocesses an image for input into the model.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Reads the image file from the provided path using **tf.io.read_file**.\n",
        " * Decodes the image into a tensor with 3 color channels **(tf.image.decode_jpeg)**.\n",
        " * Resizes the image to the target dimensions **(IMAGE_SIZE)**.\n",
        " * Converts the image values to floating-point numbers normalized between 0 and 1 **(tf.image.convert_image_dtype)**.\n",
        "\n",
        "* **Returns:** A processed image tensor ready for input into the CNN.\n",
        "\n",
        "###process_input Function:\n",
        "\n",
        "* **Purpose:** Prepares an image and its corresponding caption for training.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Calls **decode_and_resize** to preprocess the image.\n",
        " * Tokenizes the caption using the text vectorizer **(vectorization)**.\n",
        "\n",
        "* **Returns:** A tuple of the processed image tensor and the tokenized caption.\n",
        "\n",
        "###flatten_data Function:\n",
        "\n",
        "* **Purpose:** Flattens nested image-caption pairs into separate lists for processing.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Iterates through the list of images and their corresponding captions.\n",
        " * For each caption of an image, duplicates the image path, ensuring alignment for training.\n",
        "\n",
        "* **Returns:** Two flattened lists:\n",
        "\n",
        " * **flattened_images:** A list of image paths, one for each caption.\n",
        " * **flattened_captions:** A list of captions, aligned with the image paths.\n",
        "\n",
        "###make_dataset Function:\n",
        "\n",
        "* **Purpose:** Creates a tf.data.Dataset pipeline for efficient data loading and preprocessing.\n",
        "\n",
        "* **Process:**\n",
        "\n",
        " * Calls **flatten_data** to prepare lists of image paths and captions.\n",
        " * Creates a TensorFlow dataset using **tf.data.Dataset.from_tensor_slices**.\n",
        " * Maps **process_input** to preprocess each image-caption pair in parallel **(num_parallel_calls=tf.data.AUTOTUNE)**.\n",
        " * Shuffles the dataset to ensure randomness during training **(dataset.shuffle)**.\n",
        " * Batches the data into chunks of size **BATCH_SIZE** and prefetches batches for efficiency **(prefetch=tf.data.AUTOTUNE)**.\n",
        "\n",
        "* **Returns:** A batched and preprocessed dataset.\n",
        "\n",
        "###Dataset Preparation:\n",
        "\n",
        "* **train_dataset:**\n",
        "\n",
        " * Calls **make_dataset** with training data **(train_data)**.\n",
        " * Produces a dataset of preprocessed images and tokenized captions for training.\n",
        "\n",
        "* **validation_dataset:**\n",
        "\n",
        " * Calls **make_dataset** with validation data **(validation_data)**.\n",
        " * Produces a dataset for evaluating the model during training."
      ],
      "metadata": {
        "id": "n3xweFOJdNcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "def process_input(img_path, caption):\n",
        "    img = decode_and_resize(img_path)\n",
        "    caption = vectorization(caption)\n",
        "    return img, caption\n",
        "\n",
        "def flatten_data(images, captions):\n",
        "    flattened_images = []\n",
        "    flattened_captions = []\n",
        "    for img, caps in zip(images, captions):\n",
        "        for cap in caps:\n",
        "            flattened_images.append(img)\n",
        "            flattened_captions.append(cap)\n",
        "    return flattened_images, flattened_captions\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    images, captions = flatten_data(images, captions)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Prepare the datasets\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "validation_dataset = make_dataset(list(validation_data.keys()), list(validation_data.values()))"
      ],
      "metadata": {
        "id": "Sl3mFLG9IeyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Compilation, Training, and Early Stopping\n",
        "\n",
        "In this section, we have focused on:\n",
        "\n",
        "* Defining a loss function suitable for sequence-to-sequence tasks.\n",
        "\n",
        "* Compiling the image captioning model with appropriate optimization and loss strategies.\n",
        "\n",
        "* Implementing early stopping to prevent overfitting and reduce training time.\n",
        "\n",
        "* Training the model while tracking performance on the validation dataset."
      ],
      "metadata": {
        "id": "yLcfuQgSIg8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Loss Function:\n",
        "\n",
        "* **cross_entropy:** Computes the loss for sequence prediction by comparing the predicted token probabilities with the ground truth tokens.\n",
        "\n",
        " *Configuration:*\n",
        "\n",
        " * **from_logits=False:** Indicates that the output probabilities are already normalized (softmax applied).\n",
        " * **reduction='none':** Computes the loss for each token individually, enabling custom weighting or masking (e.g., ignoring padding tokens).\n",
        "\n",
        "###Instantiating the Image Captioning Model:\n",
        "\n",
        "* **caption_model:**\n",
        "\n",
        " * Combines the previously defined components **(cnn_model, encoder, decoder,** and **image_augmentation)** into the custom **ImageCaptioningModel**.\n",
        " * Ensures the entire image captioning pipeline is encapsulated in a single model for training and inference.\n",
        "\n",
        "###Compiling the Model:\n",
        "\n",
        "* **Purpose:** Specifies the optimizer and loss function for training.\n",
        "\n",
        "* **Configuration:**\n",
        "\n",
        " * **optimizer:** Adam optimizer with a learning rate of **1e-4**\n",
        " * **loss:** Uses the **cross_entropy** function for token-by-token comparison.\n",
        "\n",
        "###Defining Early Stopping:\n",
        "\n",
        "* **early_stopping:** A Keras callback to prevent overfitting by stopping training early if validation performance does not improve.\n",
        "\n",
        " *Configuration:*\n",
        "\n",
        " * **patience=3:** Waits for three epochs of no improvement before stopping.\n",
        " * **restore_best_weights=True:** Restores the model weights to the best-performing epoch for validation.\n",
        "\n",
        "###Training the Model:\n",
        "\n",
        "* **caption_model.fit:** Trains the model on the **train_dataset** and evaluates on the **validation_dataset** at each epoch.\n",
        "\n",
        " *Configuration:*\n",
        "\n",
        " * **epochs=EPOCHS:** Specifies the total number of training epochs (30 in this case).\n",
        " * **validation_data:** Provides the validation dataset for monitoring model performance.\n",
        " * **callbacks=[early_stopping]:** Uses the early stopping mechanism to improve training efficiency.\n",
        "\n",
        "###Tracking Training History:\n",
        "\n",
        "* **history:**\n",
        "\n",
        " * Captures the training and validation loss and accuracy metrics over all epochs.\n",
        " * Useful for visualizing the model's performance trends during training.\n"
      ],
      "metadata": {
        "id": "KLCYG7iQfR6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')"
      ],
      "metadata": {
        "id": "KEokAnmoIpug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Image Captioning Model\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    image_aug=image_augmentation\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "caption_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=cross_entropy\n",
        ")\n",
        "\n",
        "# Define early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "bxa-vNWRIvzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Saving, Vocabulary Exporting, and Model Reloading\n",
        "\n",
        "In this section, we have:\n",
        "\n",
        "* Saved the trained model to disk for future use.\n",
        "\n",
        "* Exported the vocabulary, enabling interpretation of model outputs.\n",
        "\n",
        "* Defined custom objects to ensure compatibility when reloading the model.\n",
        "\n",
        "* Reloaded the model to demonstrate successful serialization and deserialization."
      ],
      "metadata": {
        "id": "Y6IPF8vKIwol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving the Model:\n",
        "\n",
        "* **caption_model.save:** Saves the trained ImageCaptioningModel to a file named caption_model_test.keras. The saved file contains:\n",
        "\n",
        " * Model architecture.\n",
        " * Trained weights.\n",
        " * Optimizer state (if any).\n",
        "\n",
        "###Extracting and Saving Vocabulary:\n",
        "\n",
        "* **Extracting Vocabulary:** Calls **vectorization.get_vocabulary()** to retrieve the vocabulary used during training, which maps integer indices to tokens.\n",
        "\n",
        "* **Saving Vocabulary:**\n",
        "\n",
        " * Uses the **pickle** library to save the vocabulary as a binary file **(vocab.pkl).**\n",
        " * This file can be loaded later to interpret model outputs (e.g., mapping token indices back to words).\n",
        "\n",
        "###Defining Custom Objects:\n",
        "\n",
        "* **Purpose:** Ensures that custom model components (e.g., **ImageCaptioningModel, TransformerEncoderBlock)** are recognized during model loading.\n",
        "\n",
        "* **Configuration:** A dictionary **(custom_objects)** maps the names of custom classes to their definitions.\n",
        "\n",
        "###Loading the Model:\n",
        "\n",
        "* **keras.models.load_model:**\n",
        "\n",
        " * Reloads the saved model **(caption_model_test.keras)** into memory for inference or further training.\n",
        " * Uses the **custom_objects** dictionary to resolve custom layers and components during deserialization."
      ],
      "metadata": {
        "id": "5NDh_IblhfBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "caption_model.save('caption_model_test.keras')"
      ],
      "metadata": {
        "id": "GJLO8nIdI2VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary from the TextVectorization layer\n",
        "vocab = vectorization.get_vocabulary()\n",
        "\n",
        "# Save the vocabulary to a file\n",
        "import pickle\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "ZDeKYyO9I26l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom objects\n",
        "custom_objects = {\n",
        "    'ImageCaptioningModel': ImageCaptioningModel,\n",
        "    'TransformerEncoderBlock': TransformerEncoderBlock,\n",
        "    'TransformerDecoderBlock': TransformerDecoderBlock,\n",
        "    'PositionalEmbedding': PositionalEmbedding\n",
        "}\n",
        "\n",
        "# Load the model\n",
        "loaded_model = keras.models.load_model('caption_model_test.keras', custom_objects=custom_objects)"
      ],
      "metadata": {
        "id": "MuKRi8EbI7nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Caption Generation Using Greedy Algorithm\n",
        "\n",
        "In this section, we have implemented a function to:\n",
        "\n",
        "* Generate captions for test images using the trained image captioning model.\n",
        "\n",
        "* Apply a greedy decoding strategy for simplicity, selecting the most probable token at each step.\n",
        "\n",
        "* Decode the predicted token indices into human-readable text using the trained vocabulary."
      ],
      "metadata": {
        "id": "MbigohseJF5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vocabulary Mapping:\n",
        "\n",
        "* **vocab:** Retrieves the trained vocabulary from the **vectorization** layer.\n",
        "\n",
        "* **INDEX_TO_WORD:** Creates a dictionary that maps token indices to their corresponding words for decoding predictions into human-readable captions.\n",
        "\n",
        "###Defining greedy_algorithm Function:\n",
        "\n",
        "* **Purpose:** Generates captions for an image using a greedy decoding strategy, where the most likely token is chosen at each step.\n",
        "\n",
        "###Greedy Decoding Steps:\n",
        "\n",
        "* **Image Preprocessing:** The input image isvdecoded and resized using **decode_and_resize** and then expanded to a batch dimension **(tf.expand_dims)** for processing by the CNN.\n",
        "\n",
        "* **Feature Extraction:** The preprocessed image is passed through the model's CNN **(caption_model.cnn_model)** to extract image features.\n",
        "\n",
        "* **Encoding Features:** The extracted features are passed to the transformer encoder **(caption_model.encoder)** to generate encoded representations of the image.\n",
        "\n",
        "* **Caption Generation:**\n",
        "\n",
        " * Starts with the token <**start**> to initialize the decoding process.\n",
        " * Iteratively:\n",
        "   1. Converts the partially generated caption into tokenized format using **vectorization**.\n",
        "   2. Computes the attention mask to handle padding tokens.\n",
        "   3. Passes the tokenized caption and encoded image features to the transformer decoder **(caption_model.decoder)**.\n",
        "   4. Selects the token with the highest probability **(np.argmax)** from the predictions.\n",
        "   5. Appends the selected token to the **decoded_caption**.\n",
        "   6. Breaks the loop if <**end**> token is predicted or the maximum length **(MAX_DECODED_SENTENCE_LENGTH)** is reached.\n",
        "\n",
        "* **Postprocessing:** Removes <**start**> and <**end**> tokens from the generated caption for readability. Returns the final decoded caption as a string.\n",
        "\n",
        "* **Parameters and Constants:**\n",
        "\n",
        " * **MAX_DECODED_SENTENCE_LENGTH:** Limits the maximum length of the generated caption to avoid overly long predictions.\n",
        " * **test_images:** A list of image paths from the test dataset to be used for caption generation."
      ],
      "metadata": {
        "id": "qM8x-fUekSlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorization.get_vocabulary()\n",
        "INDEX_TO_WORD = {idx: word for idx, word in enumerate(vocab)}\n",
        "MAX_DECODED_SENTENCE_LENGTH = SEQ_LENGTH - 1\n",
        "test_images = list(test_data.keys())\n",
        "\n",
        "def greedy_algorithm(image):\n",
        "    # Read the image from the disk\n",
        "    image = decode_and_resize(image)\n",
        "\n",
        "    # Pass the image to the CNN\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    image = caption_model.cnn_model(image)\n",
        "\n",
        "    # Pass the image features to the Transformer encoder\n",
        "    encoded_img = caption_model.encoder(image, training=False)\n",
        "\n",
        "    # Generate the caption using the Transformer decoder\n",
        "    decoded_caption = \"<start> \"\n",
        "    for i in range(MAX_DECODED_SENTENCE_LENGTH):\n",
        "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
        "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
        "        predictions = caption_model.decoder(tokenized_caption, encoded_img, training=False, mask=mask)\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = INDEX_TO_WORD[sampled_token_index]\n",
        "        if sampled_token == \"<end>\":\n",
        "            break\n",
        "        decoded_caption += \" \" + sampled_token\n",
        "\n",
        "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
        "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
        "\n",
        "    return decoded_caption"
      ],
      "metadata": {
        "id": "OIXHSVbeJGcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the Caption Generation with a Sample Image\n",
        "\n",
        "In this section, we have tested the captioning model by:\n",
        "\n",
        "* Applying the greedy_algorithm function to a real image.\n",
        "\n",
        "* Verifying the generated caption text against the visual content of the image.\n",
        "\n",
        "* Displaying the image for manual evaluation of the model's performance."
      ],
      "metadata": {
        "id": "h6S1FOSnJICL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img = \"/content/drive/MyDrive/Colab/000000000785.jpg\"\n",
        "caption = greedy_algorithm(img)\n",
        "print(f'Generated Caption: {caption}\\n')\n",
        "Image.open(img)"
      ],
      "metadata": {
        "id": "A8THWGDEJIh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}